{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioBERT testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngulu864\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ngulu864\\.cache\\huggingface\\hub\\models--microsoft--BiomedNLP-BiomedBERT-base-uncased-abstract. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "import torch\n",
    "import string\n",
    "\n",
    "# Ensure you have downloaded the required NLTK resources\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ')]\n",
    "    text = \" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to classify input text\n",
    "def classify_text(text):\n",
    "    # Clean the input text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Tokenize the cleaned text\n",
    "    encodings = tokenizer(cleaned_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # Map prediction to label\n",
    "    label = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the input sentence is: Negative\n",
      "The sentiment of the input sentence is: Negative\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"it is unknown how long the virus may have been circulating, this may in part be due to the lack of early clinical recognition of an infection with which South Africa previously gained little experience during the ongoing global outbreak, potential pauci-symptomatic manifestation of the disease, or delays in care-seeking behaviour due to limited access to care or fear of stigma.\"\n",
    "result = classify_text(input_text)\n",
    "print(f\"The sentiment of the input sentence is: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the input sentence is: Negative\n",
      "The sentiment of the input sentence is: Negative\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"The International Health Regulations (IHR) National Focal Point (NFP) of the Republic of South Africa notified WHO of 20 confirmed mpox cases between 8 May and 2 July 2024, including three deaths (case fatality ratio (CFR) of 15%)\"\n",
    "result = classify_text(sample_text)\n",
    "print(f\"The sentiment of the input sentence is: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'O' (Outside): This label indicates that the token does not belong to any entity or specific relation of interest. It is outside of any target category.\n",
    "\n",
    "'B-Cause' (Beginning of Cause): This label marks the beginning of a span that represents a causal factor. It is the first token of a cause-related phrase.\n",
    "\n",
    "'I-Cause' (Inside of Cause): This label is used for tokens that are inside a span that represents a causal factor. It follows the 'B-Cause' label and continues the cause-related phrase.\n",
    "\n",
    "'B-Effect' (Beginning of Effect): This label marks the beginning of a span that represents an effect. It is the first token of an effect-related phrase.\n",
    "\n",
    "'I-Effect' (Inside of Effect): This label is used for tokens that are inside a span that represents an effect. It follows the 'B-Effect' label and continues the effect-related phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The lack of early clinical recognition of an infection leads to community transmission of mpox.\",\n",
    "    \"It is raining outside.\",\n",
    "    \"Delays in care-seeking behaviour due to limited access to care or fear of stigma.\",\n",
    "    \"The power went out because of the storm.\"\n",
    "]\n",
    "\n",
    "# Labels must align with tokens after tokenization. Here is a simplified example:\n",
    "labels = [\n",
    "    ['O', 'O', 'O', 'O', 'O', 'B-Cause', 'I-Cause', 'I-Cause', 'I-Cause', 'I-Cause', 'O', 'O', 'B-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'O', 'O', 'B-Effect', 'I-Effect', 'O'],\n",
    "    ['O', 'O', 'O'],\n",
    "    ['O', 'O', 'O', 'O', 'O', 'O', 'B-Cause', 'I-Cause', 'I-Cause', 'O', 'B-Effect', 'I-Effect', 'O'],\n",
    "    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Cause', 'O', 'O', 'O', 'B-Effect', 'O']\n",
    "]\n",
    "\n",
    "class CausalRelationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        \n",
    "        # Convert labels to IDs, padding or truncating as necessary\n",
    "        label_ids = [self.label_to_id(label) for label in labels]\n",
    "        label_ids += [self.label_to_id('O')] * (self.max_length - len(label_ids))\n",
    "        label_ids = label_ids[:self.max_length]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def label_to_id(self, label):\n",
    "        label_map = {'O': 0, 'B-Cause': 1, 'I-Cause': 2, 'B-Effect': 3, 'I-Effect': 4}\n",
    "        return label_map[label]\n",
    "\n",
    "dataset = CausalRelationDataset(texts, labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_causal_relations(text, model, tokenizer):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2).flatten().tolist()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten().tolist())\n",
    "    prediction_labels = [id_to_label(pred) for pred in predictions]\n",
    "\n",
    "    return list(zip(tokens, prediction_labels))\n",
    "\n",
    "def id_to_label(label_id):\n",
    "    label_map = {0: 'O', 1: 'B-Cause', 2: 'I-Cause', 3: 'B-Effect', 4: 'I-Effect'}\n",
    "    return label_map[label_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "sample_text = \"The sudden appearance of unlinked cases of mpox in South Africa without a history of international travel, the high HIV prevalence among confirmed cases, and the high case fatality ratio suggest that community transmission is underway, and the cases detected to date represent a small proportion of all mpox cases that might be occurring in the community; it is unknown how long the virus may have been circulating. This may in part be due to the lack of early clinical recognition of an infection with which South Africa previously gained little experience during the ongoing global outbreak, potential pauci-symptomatic manifestation of the disease, or delays in care-seeking behaviour due to limited access to care or fear of stigma.\"\n",
    "predictions = predict_causal_relations(sample_text, model, tokenizer)\n",
    "\n",
    "# Print all tokens and their corresponding labels\n",
    "for token, label in predictions:\n",
    "    print(f'{token} - {label}')\n",
    "\n",
    "# Print only tokens with labels other than 'O'\n",
    "print(\"\\nFiltered Predictions (labels other than 'O'):\")\n",
    "for token, label in predictions:\n",
    "    if label != 'O':\n",
    "        print(f'{token} - {label}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
