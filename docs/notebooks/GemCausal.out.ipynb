{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ],
   "id": "2467c9b1-962a-4ebd-914c-4b10a69798b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    cohen_kappa_score,\n",
    "    classification_report,\n",
    ")\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from dotenv import load_dotenv"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "who_data = pd.read_csv(\"../data/corpus.csv\")\n",
    "sub_data = who_data[0:9]\n",
    "sub_data"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIAPI:\n",
    "    def __init__(self, settings):\n",
    "        self.settings = settings\n",
    "\n",
    "    def gpt_response(self, message):\n",
    "        try:\n",
    "            response = openai.Completion.create(\n",
    "                prompt=message,\n",
    "                temperature=self.settings[\"temperature\"],\n",
    "                max_tokens=self.settings[\"max_tokens\"],\n",
    "                model=self.settings[\"model\"],\n",
    "            )\n",
    "            gpt_answer = response[\"choices\"][0][\"text\"].strip(\"\\n\").lower()\n",
    "            return gpt_answer\n",
    "\n",
    "        except openai.error.RateLimitError as e:\n",
    "            # Handle rate limit error\n",
    "            print(f\"Rate limited. Error message: {e}\")\n",
    "            # Wait for 60 seconds before retrying\n",
    "            print(\"Waiting for 60 seconds to start again...\")\n",
    "            time.sleep(60)\n",
    "            # Retry the API call\n",
    "            return self.gpt_response(message)\n",
    "\n",
    "    def chatgpt_response(self, message):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.settings[\"model\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": message}],\n",
    "            )\n",
    "            chatgpt_answer = (\n",
    "                response[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\").lower()\n",
    "            )\n",
    "            return chatgpt_answer\n",
    "        except openai.error.RateLimitError as e:\n",
    "            # Handle rate limit error\n",
    "            print(f\"Rate limited. Error message: {e}\")\n",
    "            # Wait for the recommended duration before retrying\n",
    "            print(\"Waiting for 60 seconds to start again...\")\n",
    "            time.sleep(60)\n",
    "            # Retry the API call\n",
    "            return self.chatgpt_response(message)\n",
    "\n",
    "    def generate_response(self, data, prompt, model, ckpt):\n",
    "        i = 0\n",
    "        result = []\n",
    "        if ckpt:\n",
    "            with open(ckpt, \"rb\") as f:\n",
    "                result = pickle.load(f)\n",
    "            i = len(result)\n",
    "            data = data[i:]\n",
    "\n",
    "        for sent in data:\n",
    "            if len(sent) == 0:\n",
    "                break\n",
    "\n",
    "            print(i)\n",
    "            message = prompt.format(sent)\n",
    "\n",
    "            if model == \"gpt3\":\n",
    "                answer = self.gpt_response(message)\n",
    "            elif model in [\"chatgpt\", \"gpt4\", \"gpt-3.5-turbo\"]:\n",
    "                answer = self.chatgpt_response(message)\n",
    "            else:\n",
    "                raise ValueError(f\"Model {model} not recognized\")\n",
    "            result.append(answer)\n",
    "            i += 1\n",
    "\n",
    "            # Save partial response\n",
    "            CKPT = \"./results/ckpt/\"\n",
    "            if not os.path.exists(CKPT):\n",
    "                print(f\"{CKPT} does not exist. Creating...\")\n",
    "                os.mkdir(CKPT)\n",
    "            with open(f\"{CKPT}{model}_result_part.pkl\", \"wb\") as f:\n",
    "                pickle.dump(result, f)\n",
    "        print(\"---Done\")\n",
    "        return result\n",
    "\n",
    "\n",
    "def load_prompt(dir):\n",
    "    with open(dir) as f:\n",
    "        prompt = \"\".join(f.readlines())\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_confusion_matrix(data, label1, label2, output_dir):\n",
    "    true, pred = data[label1], data[label2]\n",
    "    true = true.to_numpy().reshape(-1)\n",
    "    pred = pred.to_numpy().reshape(-1)\n",
    "    mt = confusion_matrix(true, pred)\n",
    "\n",
    "    with open(output_dir.format(\"confusion_matrix\"), \"a\") as f:\n",
    "        f.write(f\"---{label1} & {label2} Matrix---\\n\")\n",
    "        f.write(np.array2string(mt) + \"\\n\\n\")\n",
    "\n",
    "\n",
    "def get_macrof1_score(data, label1, label2, output_dir):\n",
    "    true, pred = data[label1], data[label2]\n",
    "    score = f1_score(true, pred, average=\"macro\")\n",
    "    report = classification_report(true, pred, digits=3)\n",
    "    print(f\"Macro f1-score between {label1} & {label2}: {round(score, 3)}\")\n",
    "    print(\"---Classification Report created\")\n",
    "\n",
    "    with open(output_dir.format(\"macro_f1score\"), \"a\") as f:\n",
    "        f.write(f\"Macro f1-score between {label1} & {label2}: {round(score, 3)}\\n\")\n",
    "        f.write(report + \"\\n\")\n",
    "\n",
    "\n",
    "def get_kappa_score(data, label1, label2, output_dir):\n",
    "    true, pred = data[label1], data[label2]\n",
    "    score = cohen_kappa_score(true, pred)\n",
    "    print(f\"Cohen Kappa score between {label1} & {label2}: {round(score, 3)}\")\n",
    "\n",
    "    with open(output_dir.format(\"cohenKappa\"), \"a\") as f:\n",
    "        f.write(f\"Cohen Kappa score between {label1} & {label2}: {round(score, 3)}\\n\")\n",
    "\n",
    "\n",
    "def main(work, model, data, column_name, prompt_text, output_path, checkpoint=None):\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"{output_path} does not exist. Creating...\")\n",
    "        os.mkdir(output_path)\n",
    "\n",
    "    settings = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 150,\n",
    "    }\n",
    "\n",
    "    if model == \"gpt3\":\n",
    "        settings[\"model\"] = \"text-davinci-003\"\n",
    "    elif model == \"gpt4\":\n",
    "        settings[\"model\"] = \"gpt-4\"\n",
    "    elif model in [\"chatgpt\", \"gpt-3.5-turbo\"]:\n",
    "        settings[\"model\"] = \"gpt-3.5-turbo\"\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model} not recognized\")\n",
    "\n",
    "    # Print column names to help identify the correct column\n",
    "    print(\"Available columns:\", data.columns)\n",
    "\n",
    "    # Check if the specified column exists in the dataset\n",
    "    if column_name not in data.columns:\n",
    "        raise KeyError(\n",
    "            f\"Column '{column_name}' does not exist in the dataset. Please check the column names.\"\n",
    "        )\n",
    "\n",
    "    if work == \"api\":\n",
    "        data = data[column_name]\n",
    "        prompt = prompt_text\n",
    "        output_file_path = os.path.join(output_path, f\"{model}-results.csv\")\n",
    "\n",
    "        api = OpenAIAPI(settings)\n",
    "        result = api.generate_response(data, prompt, model, checkpoint)\n",
    "\n",
    "        df = pd.DataFrame({column_name: data, f\"{model}-results\": result})\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "        print(\"Saved result file\")\n",
    "\n",
    "    elif work == \"quant\":\n",
    "        colnames = data.columns[1:]\n",
    "        output_file_path_template = os.path.join(output_path, f\"{model}-quant.txt\")\n",
    "        for label1, label2 in combinations(colnames, 2):\n",
    "            if label1 == \"label\":\n",
    "                get_confusion_matrix(data, label1, label2, output_file_path_template)\n",
    "                get_macrof1_score(data, label1, label2, output_file_path_template)\n",
    "                get_kappa_score(data, label1, label2, output_file_path_template)\n",
    "            elif label1 != \"label\":\n",
    "                get_kappa_score(data, label1, label2, output_file_path_template)"
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "work = \"api\"  # or 'quant'\n",
    "model = \"gpt-3.5-turbo\"  # or 'chatgpt', 'gpt4'\n",
    "prompt_text = \"Extract all causes and effects pairs that are drivers of the emergence and transmission of the disease: {}\"\n",
    "output_path = \"./results/\"\n",
    "checkpoint = None  # or specify a checkpoint path\n",
    "column_name = \"Text\"  # specify the column to be used for the analysis\n",
    "\n",
    "# Call the main function\n",
    "main(work, model, sub_data, column_name, prompt_text, output_path, checkpoint)"
   ],
   "id": "cell-3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
