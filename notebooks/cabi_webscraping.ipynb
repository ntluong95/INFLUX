{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jsonlines'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n",
      "\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjsonlines\u001b[39;00m\n",
      "\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jsonlines'"
     ]
    }
   ],
   "source": [
    "#%% 1. Loading packages\n",
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "import jsonlines\n",
    "import random\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import HTTPError, RequestException, ConnectionError, ReadTimeout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define WriteMode for file handling\n",
    "class WriteMode:\n",
    "    APPEND = 'a'\n",
    "    OVERWRITE = 'w'\n",
    "\n",
    "def write_jsonl(input_entries: list[dict], output_file: str, write_mode: WriteMode):\n",
    "    \"\"\"Write content to jsonl format.\"\"\"\n",
    "    folder = Path(output_file).parent\n",
    "    if not folder.exists():\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    with jsonlines.open(output_file, write_mode) as f:\n",
    "        for entry in input_entries:\n",
    "            f.write(entry)\n",
    "\n",
    "#%% 2. Define Crawler class\n",
    "class Crawler:\n",
    "    \"\"\"Crawler class for extracting data from the CABI Digital Library.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = self.get_session()\n",
    "        self.data_buffer = []\n",
    "    \n",
    "    def get_session(self, total_retries: int = 3, backoff_factor: float = 0.1, status_forcelist: list[int] = [500, 502, 503, 504, 429]):\n",
    "        \"\"\"Generate a session object with retry settings.\"\"\"\n",
    "        retries = requests.packages.urllib3.util.retry.Retry(total=total_retries, backoff_factor=backoff_factor, status_forcelist=status_forcelist)\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        session = requests.Session()\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "        return session\n",
    "        \n",
    "    def get_url(self, url: str, timeout: int = 300):\n",
    "        \"\"\"Get the response from a URL.\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except (HTTPError, RequestException, ConnectionError, ReadTimeout) as err:\n",
    "            print(f'Error when connecting: {err}')\n",
    "            return None\n",
    "    \n",
    "    def extract_data(self, url: str, output_file: str):\n",
    "        \"\"\"Extract data from the given URL and write to output.\"\"\"\n",
    "        response = self.get_url(url)\n",
    "        if not response:\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Example of extracting specific elements - adjust as needed for your use case\n",
    "        articles = soup.find_all('div', class_='search-result__body')  # Adjust based on actual HTML structure\n",
    "        buffer = []\n",
    "\n",
    "        for article in articles:\n",
    "            title = article.find('h2').get_text().strip() if article.find('h2') else 'No title'\n",
    "            authors = article.find('div', class_='meta__authors').get_text().strip() if article.find('div', class_='meta__authors') else 'No authors'\n",
    "            date = article.find('div', class_='meta__date').get_text().strip() if article.find('div', class_='meta__date') else 'No date'\n",
    "            abstract_preview = article.find('div', class_='abstract-preview').get_text().strip() if article.find('div', class_='abstract-preview') else 'No abstract preview'\n",
    "            \n",
    "            # Adding more details if available\n",
    "            full_text_link = article.find('a', text='FULL TEXT')['href'] if article.find('a', text='FULL TEXT') else None\n",
    "            pdf_link = article.find('a', text='PDF/EPUB')['href'] if article.find('a', text='PDF/EPUB') else None\n",
    "            \n",
    "            result = {\n",
    "                'title': title,\n",
    "                'authors': authors,\n",
    "                'date': date,\n",
    "                'abstract_preview': abstract_preview,\n",
    "                'full_text_link': full_text_link,\n",
    "                'pdf_link': pdf_link\n",
    "            }\n",
    "            buffer.append(result)\n",
    "        \n",
    "        write_jsonl(buffer, output_file, WriteMode.APPEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 3. Running the crawler\n",
    "if __name__ == '__main__':\n",
    "    base_url = \"https://www-cabidigitallibrary-org.ezp.sub.su.se/action/doSearch?ConceptID=500060&startPage=0&sortBy=Earliest\"\n",
    "    output_file = 'cabi_data.jsonl'\n",
    "    crawler = Crawler()\n",
    "    \n",
    "    # Extract data from the base URL\n",
    "    crawler.extract_data(base_url, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
